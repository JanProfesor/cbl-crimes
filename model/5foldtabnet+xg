# REQUIREMENTS:
# pip install pandas scikit-learn torch pytorch-tabnet optuna tqdm xgboost

import json
import numpy as np
import pandas as pd
import torch
import xgboost as xgb
import optuna
from optuna.pruners import HyperbandPruner
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from pytorch_tabnet.tab_model import TabNetRegressor


# ―――――――――――――――――――――――――――
# 0. Load & preprocess
# ―――――――――――――――――――――――――――
df = pd.read_csv("processed/final_dataset_residential_burglary.csv")
df['date'] = pd.to_datetime(df[['year','month']].assign(day=1))
df = df.sort_values(['ward_code','date'])

TARGET = 'burglary_count'
for lag in (1, 3, 6):
    df[f'{TARGET}_lag_{lag}'] = (
        df.groupby('ward_code')[TARGET].shift(lag)
    )
# cyclical month features
df['month_num'] = df['date'].dt.month
ang = 2 * np.pi * df['month_num'] / 12
df['month_sin'] = np.sin(ang)
df['month_cos'] = np.cos(ang)

# drop rows with missing lags
df.dropna(
    subset=[f'{TARGET}_lag_{l}' for l in (1, 3, 6)],
    inplace=True
)

X = df.drop(columns=[TARGET, 'date']).reset_index(drop=True)
y = df[TARGET].values.reshape(-1, 1)

# categorical encoding for TabNet
X['ward_code'] = X['ward_code'].astype('category').cat.codes
cat_idxs = [X.columns.get_loc('ward_code')]
cat_dims = [X['ward_code'].nunique()]

# final 70/30 hold-out split (no shuffle)
X_full, X_hold, y_full, y_hold = train_test_split(
    X.values, y, test_size=0.3, shuffle=False
)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# ―――――――――――――――――――――――――――
# 1. Baseline 5-fold CV
# ―――――――――――――――――――――――――――
print("\n=== Baseline 5-fold CV (fixed params) ===")
tscv = TimeSeriesSplit(n_splits=5)
for fold, (tr, val) in enumerate(tscv.split(X_full), 1):
    Xt, Xv = X_full[tr], X_full[val]
    yt, yv = y_full[tr], y_full[val]
    m = TabNetRegressor(
        n_d=16, n_a=16, n_steps=5, gamma=1.5,
        cat_idxs=cat_idxs, cat_dims=cat_dims, cat_emb_dim=1,
        optimizer_fn=torch.optim.Adam,
        optimizer_params={'lr': 1e-2},
        scheduler_fn=None,
        mask_type='sparsemax',
        device_name=device
    )
    m.fit(
        Xt, yt,
        eval_set=[(Xv, yv)],
        eval_name=['val'],
        eval_metric=['rmse'],
        max_epochs=30,
        patience=5,
        batch_size=512,
        virtual_batch_size=64
    )
    p = m.predict(Xv).reshape(-1)
    print(
        f" Fold {fold}: "
        f"RMSE={np.sqrt(mean_squared_error(yv,p)):.3f}, "
        f"MAE={mean_absolute_error(yv,p):.3f}, "
        f"R²={r2_score(yv,p):.3f}"
    )

# ―――――――――――――――――――――――――――
# 2. Optuna tuning (5-fold inside)
# ―――――――――――――――――――――――――――
pruner = HyperbandPruner(min_resource=5, max_resource=20, reduction_factor=3)
study = optuna.create_study(direction="minimize", pruner=pruner)

def objective(trial):
    # TabNet hyperparams
    n_d       = trial.suggest_int('n_d', 8, 64)
    n_steps   = trial.suggest_int('n_steps', 3, 10)
    gamma     = trial.suggest_float('gamma', 1.0, 2.5)
    lr        = trial.suggest_float('lr', 1e-4, 1e-1, log=True)
    wd        = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)
    bs        = trial.suggest_categorical('batch_size', [128, 256, 512])
    optimizer = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])
    sched     = trial.suggest_categorical('scheduler', ['none', 'StepLR'])
    # XGBoost hyperparams
    xgb_lr    = trial.suggest_float('xgb_lr', 0.01, 0.3, log=True)
    xgb_md    = trial.suggest_int('xgb_max_depth', 3, 8)
    xgb_sub   = trial.suggest_float('xgb_subsample', 0.5, 1.0)
    blend_w   = trial.suggest_float('blend_weight', 0.0, 1.0)

    # map optimizer
    opt_fn = torch.optim.Adam if optimizer == 'Adam' else torch.optim.RMSprop
    sched_fn = (
        None
        if sched == 'none'
        else torch.optim.lr_scheduler.StepLR
    )
    sched_params = {'step_size':10, 'gamma':0.8} if sched=='StepLR' else {}

    rmses, maes, r2s = [], [], []
    tscv = TimeSeriesSplit(n_splits=5)
    for tr, val in tscv.split(X_full):
        Xt, Xv = X_full[tr], X_full[val]
        yt, yv = y_full[tr], y_full[val]

        # --- TabNet model ---
        tn = TabNetRegressor(
            n_d=n_d, n_a=n_d, n_steps=n_steps, gamma=gamma,
            cat_idxs=cat_idxs, cat_dims=cat_dims, cat_emb_dim=1,
            optimizer_fn=opt_fn,
            optimizer_params={'lr': lr, 'weight_decay': wd},
            scheduler_fn=sched_fn,
            scheduler_params=sched_params,
            mask_type='sparsemax',
            device_name=device
        )
        tn.fit(
            Xt, yt,
            eval_set=[(Xv,yv)],
            eval_name=['val'],
            eval_metric=['rmse'],
            max_epochs=20,
            patience=3,
            batch_size=bs,
            virtual_batch_size=64
        )
        p_tn = tn.predict(Xv).reshape(-1)

        # --- XGBoost model ---
        dtrain = xgb.DMatrix(Xt, yt)
        dvalid = xgb.DMatrix(Xv, yv)
        params = {
            'objective': 'reg:squarederror',
            'learning_rate': xgb_lr,
            'max_depth': xgb_md,
            'subsample': xgb_sub,
            'tree_method': 'hist',
            'gpu_id': 0 if device=='cuda' else -1,
        }
        xgb_model = xgb.train(
            params, dtrain,
            num_boost_round=200,
            evals=[(dvalid,'val')],
            early_stopping_rounds=10,
            verbose_eval=False
        )
        p_xgb = xgb_model.predict(dvalid)

        # blended prediction
        p = blend_w * p_tn + (1 - blend_w) * p_xgb

        rmses.append(np.sqrt(mean_squared_error(yv, p)))
        maes .append(mean_absolute_error(yv, p))
        r2s .append(r2_score(yv, p))

    rmse_mean, mae_mean, r2_mean = np.mean(rmses), np.mean(maes), np.mean(r2s)
    trial.set_user_attr("avg_mae", mae_mean)
    trial.set_user_attr("avg_r2",  r2_mean)
    return float(rmse_mean)

print("\n=== Running Optuna (tuning TabNet + XGB + blend) ===")
study.optimize(objective, n_trials=20, timeout=2*60*60)  # or n_trials=50

# save trials & best
tr_df = study.trials_dataframe(
    attrs=('number','value','params','user_attrs')
)
tr_df.to_csv("optuna_trials.csv", index=False)
with open("optuna_best.json","w") as f:
    json.dump({
        "best_rmse": study.best_value,
        "best_params": study.best_trial.params,
        "best_mae": study.best_trial.user_attrs['avg_mae'],
        "best_r2": study.best_trial.user_attrs['avg_r2']
    }, f, indent=2)

print(f"\nBest trial #{study.best_trial.number}:")
print(f" RMSE = {study.best_value:.4f}")
print(f" MAE  = {study.best_trial.user_attrs['avg_mae']:.4f}")
print(f" R²   = {study.best_trial.user_attrs['avg_r2']:.4f}")
print(" Params:", study.best_trial.params)

# ―――――――――――――――――――――――――――
# 3. Final retrain & evaluate on hold-out
# ―――――――――――――――――――――――――――
print("\n=== Final ensemble on hold-out ===")
bp = study.best_trial.params

# 3.1 TabNet full
tabnet = TabNetRegressor(
    n_d=bp['n_d'], n_a=bp['n_d'], n_steps=bp['n_steps'], gamma=bp['gamma'],
    cat_idxs=cat_idxs, cat_dims=cat_dims, cat_emb_dim=1,
    optimizer_fn=(torch.optim.Adam if bp['optimizer']=='Adam' else torch.optim.RMSprop),
    optimizer_params={'lr':bp['lr'],'weight_decay':bp['weight_decay']},
    scheduler_fn=(None if bp['scheduler']=='none' else torch.optim.lr_scheduler.StepLR),
    scheduler_params={'step_size':10,'gamma':0.8} if bp['scheduler']=='StepLR' else {},
    mask_type='sparsemax', device_name=device
)
tabnet.fit(
    X_full, y_full,
    max_epochs=30,
    patience=5,
    batch_size=bp['batch_size'],
    virtual_batch_size=64
)
pred_tab = tabnet.predict(X_hold).reshape(-1)

# 3.2 XGBoost full
dtrain = xgb.DMatrix(X_full, y_full)
dhold  = xgb.DMatrix(X_hold, y_hold)
xgb_params = {
    'objective': 'reg:squarederror',
    'learning_rate': bp['xgb_lr'],
    'max_depth': bp['xgb_max_depth'],
    'subsample': bp['xgb_subsample'],
    'tree_method': 'hist',
    'gpu_id': 0 if device=='cuda' else -1,
}
xgb_model = xgb.train(
    xgb_params, dtrain,
    num_boost_round=500,
    evals=[(dhold,'hold')],
    early_stopping_rounds=20,
    verbose_eval=False
)
pred_xgb = xgb_model.predict(dhold)

# 3.3 Blend & report
bw = bp['blend_weight']
pred_ens = bw * pred_tab + (1 - bw) * pred_xgb

rmse_e = np.sqrt(mean_squared_error(y_hold, pred_ens))
mae_e  = mean_absolute_error(y_hold, pred_ens)
r2_e   = r2_score(y_hold, pred_ens)
print(f"Ensemble RMSE = {rmse_e:.3f}")
print(f"Ensemble MAE  = {mae_e:.3f}")
print(f"Ensemble R²   = {r2_e:.3f}")

with open("ensemble_metrics.json","w") as f:
    json.dump({"rmse":rmse_e,"mae":mae_e,"r2":r2_e}, f, indent=2)
print("✅ Saved optuna_trials.csv, optuna_best.json and ensemble_metrics.json")
